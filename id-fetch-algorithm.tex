\subsection{Selecting a representative sample of videos from YouTube}

%TO WRITE: SOME INTRO WHY WE WANT TO GET VIDEOS, WHAT IS OUR AIM
%- chosen parameter
%- unbiased
%- large dataset

Before we present our implementation we want to give a brief summary of
different kinds of strategies to receive data from YouTube since there might be
some limitations and choosing a strategy depends on various individual decision
criteria.

\subsubsection{Crawling the YouTube website}
An unbiased set of videos and metadata may be a desire of, or indeed a
requirement for, a statistical analyst. YouTube's recommendation system is
tailored to provide the most relevant content to one user during a single
session, or over the course of multiple sessions. Up to 20 recommended videos are shown per video page. Among these are paid
promotions by other users or networks, top trending videos, and videos directly related to the last
submitted search query, (probably) amongst others. If one was to naively crawl
through content served to them by the YouTube recommendation system one would
certainly not venture far from the starting point due to an inherent
confirmation bias in algorithms typically chosen for this
purpose~\cite{youtube-recommendation-system}: YouTube will
continue to serve videos related to the ones one is already watching, thus it is
difficult to break out of even a certain category of content. One might think of
this as actively hindering us in our hunt for an unbiased representative dataset.

\subsubsection{Random generation of video IDs}
We considered for a moment attempting to create a brute force solution to
create truly unique and random video IDs and trying as many of these as
possible, but there is a tiny problem with that approach:
Assume that YouTube uses a modified Base64 encoding for URLs with 11 digits
for their video IDs. A ballpark (insane over-)estimation for the total amount of videos on
YouTube is 300 hours per minute since Google's acquisition in 2006 (9 years).
If we estimate the average video to be five minutes long, that leaves us with
\[frac{85\,140\,000\,000}{5} = 17\,028\,000\,000\] videos. With 64 to the power
of 11 possible video IDs we would have to generate more than four billion random
IDs before we could expect to get a single hit. Not feasable.

\subsubsection{The YouTube API}
YouTube provides a REST\footnote{Representational State Transfer: http://www.jopera.org/docs/publications/2008/restws} Application Programming Interface (API)~\cite(youtube-api)
which can be
queried for information with a set of parameters, in absence of a standard
website interface to utilize (like the one provided by
\texttt{www.youtube.com}). The API consists of different
endpoints, and the one used for searching videos is called "search.list". This
endpoint returns a result as a JSON object. It accepts a rich list
of customizable parameters as input allowing users to create highly specific and
optimized queries to achieve the best possible efficiency for the specific use
case.

For a given query, the search.list response will contain at most 50 videos. There
may be multiple pages, as indicated by a \texttt{nextPageToken} in the JSON
object, but no more than ten pages are returned for any one set of parameters.
To get a complete list of video IDs for a given set of parameters the chosen
query has to be split up into subqueries that by themselves does not match more
than 500 videos. This splitting, and how we recommend doing it, is discussed in
detail in this section.

The main problem with
the 500 video limit for a static parameter set is that it can not be a good
sample. There might be a lot of other videos which match the requested
parameter, but subsequent requests to the API only return more or less the exact
same sample. There are minor variations in the returned sample, and we will 
discuss this and the related issues in more detail later. 

As a side note it should be mentioned that using an actual web crawler will have
properties an API reliant fetcher can not recreate. A crawler will have its 
behaviour logged by YouTube, which in turn provides more content in line with
what it thinks the crawler likes. The resulting set of videos will be more like
a set of videos a given user is likely to see, it is inherently biased. For some
cases such a set of videos might be desirable, for instance if one wants to
focus on popular videos, or if one wants to measure how biased a set becomes
over time.

Before continuing a few terms will have to be clarified. A  "static parameter 
set" is the set of parameters that are static, globally, for a set of queries, 
like "All videos which are related to the word 'fun', that are 2D and have a 
high video quality". A "variable parameter" is a parameter that can be changed 
for every single request in a request chain, while the static parameter set 
remains untouched. This results in the ability to create many different
variations of the "static parameter set" in order to exceed the 500 videos 
maximum.

From the search.list API endpoint only four different variables can be varied 
between requests in a chain. All other parameters are in some kind static and
would result in a maximum of 500 videos. Following is a description of the
variable parameters.

\subsubsection{location and locationRadius}
The problem with this parameter is that not every video on YouTube has specified
the location in the metadata. Evaluating some hundred thousands of videos
metadata has shown that only 5-10\% of the videos has specified a location in
their metadata. We can not verify that this is the average on all videos
uploaded on YouTube, but just the fact that some (a lot) of videos lack data in
this field indicates that trying to vary it to get an unbiased sample will
result in a set of videos that be default leaves out a lot of videos. For
unbiasedness this is sub-optimal, but not disastrous. It is reasonable to
assume that the set of videos with location data contains a well distributed
and unbiased subset, but this can not easily be verified.

\subsubsection{channelId}
In order to use this parameter as a means of getting an unbiased set of videos,
we would need to have a list of all channels available on YouTube. This is as
difficult as getting all videos of YouTube and therefore varying this parameter 
is probably not a good approach to get an unbiased set of videos. Some channels
also have some thousands of videos uploaded, so with a static parameter set and
only varying the channelId, the API response would be limited to 500 videos. One
could argue that 500 videos is a representative set of videos for a channel, but
the problem is that this subset of the channel's full video list is provided by
YouTube, outside our control. Thus the returned sample can not safely be assumed
to be representative.

\subsubsection{q - search term}
In theory this parameter could be varied and cycled through all possible
combinations of symbols (words, in a wide sense), and through that get an
unbiased set of videos. This is not only unfeasible due to the remarkably big
list of words one would need\footnote{171\,000 words in english alone, then add
all other languages, not to mention names and other word-constructs}, but for
some cases the API quota cost would be extremely high measured as cost per
result. For instance, if one was to get an unbiased set of 3D videos, one would
have to cycle through the whole word list and end up spending quota points on queries
that return stale results. In addition to this, if one is to vary the
search term, the user is left without the ability to manually narrow the result
set. We want to allow the user the ability to create sets of data tailored to
specific tasks, should it be required.

\subsubsection{publishedAfter and publishedBefore}
By having a date range as the varying parameter for a search query, and assuming
that the API actually returns all videos matching the query, the resulting set
would be unbiased with regards to all aspects except from time. Of course, for
a big timeframe there exists more than 500 videos, so to circumvent this cap,
the timeframe can be recursively sliced up until it is so small that less than
500 videos match the parameters. Varying this way leaves the rest of the
parameters unchanged, and the resulting set becomes as much as possible
unbiased.

By slicing the timeframe up like this, one will also be able to get more than
500 videos from a given channel, or from within a geographic zone. It becomes
apparent that varying this parameter alone is the best way to achieve a unbiased
set of videos. A nice bonus is that it also allows customization of the query
without leaving videos out of the resulting set.

