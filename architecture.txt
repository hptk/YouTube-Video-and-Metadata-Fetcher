\section{Architecture}
As an architecture for our tool we chose a client-server model with a REST API. The proposed solution does not only allow the tool to be 
used by multiple users at the same time but also enables it to be distributed among several computers with different roles, responsibilities 
and hardware resources. The user's frontend is a thin client, which only initiates processes on the server and receive the corresponding
results whereas the server needs more hardware resources for obtaining, processing and saving the data available from the YouTube API.
A local deployment on a single computer is also practicable and it should not dramatically influence the tool's performance as long as
a sufficient internet connectivity is guaranteed since most of the implemented processes on the server side are network I/O bound.


\subsection{Client}
The tool's user interface is written in AngularJS, which is an open-source framework from Google, allowing to create single page web applications by 
providing a client-server model-view-controller (MVC) pattern.~\cite{architecture:angularjs} 

The frontend is separated into four main pages which mirror the process from creating a search query until viewing some statistics about the received data from 
the YouTube API.

\subsubsection{User management}


\subsubsection{API key management}
On the API key management the user can add multiple API keys to his account. Each API key will be validated by the client on the fly to
ensure that only available keys can be selected on the second page, the query builder.
While the YouTube API only requires the existens of one key in each request to be accessible, we found it quite useful having the possibility to store multiple
keys in the database for a single user. So the user has the ability to manage different key for different kinds of search queries and can
filter the informations displayed on the other pages based on a selected key.

\subsubsection{Query Builder}
On this page the user has the possibility to define his search query. Based on our algorithm and the YouTube API requirements, there are
three required fields.The API key, publishedAfter and publishedBefore parameter.
In a hidden collapsed section we have integrated almost any parameters provided by the YouTube API search.list endpoint as optional fields, 
such as video dimension, video duration, relevance language and much more. Every field has a popover displaying the description from the YouTube
API documentation.
Having such a big set of parameters, the user is able to build complex and highly variable search queries.
Actually some parameters provided by the API are omitted because they are only usable to search own audio visual content uploaded on YouTube.

Before a query is send to the server and stored in the database, it will be validated by creating a request to the YouTube API whereby
the user gets a real time error description if the current composition of parameters is invalid. This client side validation is useful 
to not only prevent invalid search queries to be stored in the databse but also to give the user a feeling for the YouTube API without having
a long introduction on how to use the query builder.

\subsubsection{Task Page}
After a user has defined his search query, he can run several tasks on the query.


\subsubsection{Result page}
On the result page we have aggregated some statistics about the videos which are received by a selected query. 
Because the aggregation can take a lot of time for a big dataset, every section on this pages does asynchronously load the result by 
requesting the server only if it is necessary. Per default there are three sections loaded on the initilization of the page. 
First, a summary of the current selected query, containing informations about how many videos were found, how many
metadata, comments and video representations from the DASH mdp file are related to the found videos and stored in the database. 
Second, a pie chart and table showing the top 10 categories. Third, a table showing 
associated queries which were created in the past but share some videos within the group of the current query's videos.
In the table there is a visualisation of the videos percentage intersection, which allows
the user to quickly identify related search queries. It might be relevant to see how multiple queries are connected to each other after
manipulating only one search parameter as we will see in the experimential results section.

For some numeric values such as the views, comments, likes, dislikes and duration we also provide statistical aggregations of the minimum,
maximum, average and standard deviation for the selected query compared to all videos saved in the database.


\subsection{Server}
The server is written in python

\subsubsection{Background tasks}
Since we already have a centralized server which processes all tasks created by the clients, such as searching for 
video IDs and extracting the metadata, we had to think about how to run these tasks in a non-blocking way.
This is important
because the users might want to create multiple tasks at once and the frontend should not be blocked until one long running task
is finished. In order to solve this problem, we had to utilize a task queue system and select a message broker to feed the task queue.
We have evaluated Celery as our task queue and Redis as a message broker as the most suitable packages available for python.

Celery is an asynchronous task queue based on distributed messages~\cite{architecture:celery}
Redis itself is an open-source, networked, in-memory key-value data structure server.~\cite{architecture:redis} For our
purpose we do not utilize the full capabilities of Celery and Redis, since we have one centralized server, but using
this architecture from the very beginning makes it easy to distribute our tool on multiple servers which process the individual tasks and only
have some light weight clients which request our API.
This might be important for performance improvements and further developments.

After the user has scheduled a task from the client user interface, the task is pushed to Redis and is in a pending state until a Celery worker is available to run the task in
the background. While running the tasks in the background, celery provides an interface to update the current task's state. At this point it is important to have a different 
result backend than the normal database where we want to store the received data from the YouTube API. Because each task updates his own state after every request to the YouTube API 
in order to provide the client real time informations about the current state of a task. This results in multiple updates every second for a single task where there 
might be multiple tasks running in concurrency. Using the database on the disk as the result backend would lead to a high
disk I/O only for sharing the task's state with the client and block the saving of the data received by the YouTube API which would dramatically slow down the complete process.
Since we already have Redis as our broker system with an in-memory data structure, we use it as the result backend for our tasks and only store the tasks informations 
in the database persistently after the task has finished.







