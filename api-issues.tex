\section{API Issues}
We have experienced that the API in some situations returns and indicates that
there are multiple pages, even if there are less than 50 videos in the result
list. The returned result also contains a field representing the number of items
returned, but this field often does not match the actual amount of items in the 
result~\cite[Issue 5173]{conclusion:gdataissue}. Returning results as lists with
open spots while at the same time indicating that more results is on the next
page is in itself illogical, as a more natural way to return results would be to
fill each page, and only when there are less than 50 videos remaining that match
the query return a non-full list. This leads to flaws or imperfections in the 
logic in the clients making queries to the API, as one can not trust either the
total count field, the presense of a next page token or the actual number of
returned results.

Our experience when querying for the next page when the list is not full, shows
that the page and all subsequent pages contain the exact same set of videos. 
This is a known issue in the API~\cite[Issue 6406]{conclusion:gdataissue}. In 
combination with the aforementioned issues this leads us to the conclusion that 
iterating over the pages a for a single static parameter set has becomes a bad 
idea because it is impossible to know if there are new videos on the next page 
or not. %TODO expand on this?

We have also found that when issuing the same request to the server several
times, the API might respond slightly differently each time. This result is
easily reproducible and also a know issue with the 
API~\cite[Issue 4275]{conclusion:gdataissue}. 

The unfortunate consequences of these issues include excess API quota spending,
excess time consumption whilst getting results from the API, increased network
and local resource consumption and uncertainty with regards to the integrity of
the results. 

As a part of our experiment we issued a lot of requests to the YouTube API to
gather data. A strange pattern quickly emerged, requests for timeslots longer
back than a few days contained drastically less returned unique videos, and 
stepping a month back, a request for all videos uploaded a given day will be 
about 98\% less than a request for videos uploaded yesterday. This decline in
returned video IDs seem to go in steps, as visualised
in~\cref{october-video-ids}. For the past three days the returned unique video
IDs are around 200\,000. The next five days back returns around 65\,000 to 
75\,000 video IDs, and the subsequent 12-14 days return 20\,000 to 30\,000
videos. From that point and backwards the number of videos per day remain
stable at around 2\,000 to 4\,000, slowly declining to only a few hundred
returned videos per day.~\cref{jan-ids-year} shows the return when asking for 
videos published in January from 2009 to 2015. 

At September 26th 2015 the API returned a total 246\,106 video IDs for that day.
For the same day, if queried a little more than a month later, the API returns
a mere 4\,394 videos.~\cref{2015-video-ids} shows how after a month has passed 
the amount of videos dramatically decline. The missing videos are seemingly 
random, everything from strange videos with very few views to relatively large 
videos (20\,000+ views) is missing in the data set when requeried a month later. 
The remaining data set also seems random, containing videos with view counts 
from very small number to very large numbers. Perhaps with more research the
nature of this video ID purging can be understood.

\subsubsection{Accounting for API inconsistances}
Contrary to natural intuition, the API may return a wholly different set of
videos for two queries one might have thought to return the same as a single
\textit{superquery}. By example: If you search for "All videos between date
$X$ and date $Y$ you might get 1\,000 results. But if you split the query into
two queries: One specifically for 2D videos and one specifically for 3D videos;
you might get a subset of the results for the single query, but in addition,
the API might return a much larger set than before.

By splitting the set timeframe in two, creating two subqueries in the process,
one can trick\footnote{To a certain extent. Our tool is by no means perfect. The
API still severely limits our search.} the API into returning more videos for a given
timeframe than the original query would have yielded. We have expanded our
possible result space simply by using unused parameters to create distinct
subqueries with increasing granularity.
Our tool will do this through narrowing the timeframe of the query (and
in the process providing a section on this on the result page), but
it would be fairly trivial to implement an algorithm that uses all possible
unused parameters and some clever pruning to increase the possible result space.

