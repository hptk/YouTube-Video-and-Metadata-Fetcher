\section{Architecture}
A client-server model with a REST API was chosen as the tool architecture. The
proposed solution allows the tool to be used by multiple users simultaneously,
as well as enabling deployment to distributed systems with distinct roles,
responsibilities, and hardware resources.
The frontend is a simple thin client whose sole job is relaying commands to a
server and showing the returned results. The server is tasked with obtaining
and processing the available YouTube data, and otherwise interacting with the
API.
Local deployment is a viable option, given sufficient storage and networking
capabilities.


\subsection{Client}
The user interface of the tool is written in AngularJS, an open-source framework
that facilitates easy setup and management of single-page web applications using
the model-view-controller (MVC) pattern.~\cite{architecture:angularjs}

The frontend is separated into four main pages that each have a specific
function: Management of API keys, creation of API queries, creation of celery
tasks, and results presentation.


\subsubsection{User management}


\subsubsection{API key management}
The API key management page lets the user register API keys to his account.
API keys are instantly validated and, if valid, added to the dropdown list of
available keys on the query builder page.
While only a single key is required per API request (in the absence of OAuth
2.0), having access to multiple keys makes for quick and easy testing and the
ability to generate keys for a specific purpose or dataset. The latter is
shown in action on the Result page, where the user can view statistics filtered
by a given API key. %TODO: refine?


\subsubsection{Query Builder}
On the Query builder page the user may build individual search queries using
the provided interface. YouTube's API defines a set of options which we present
to the user in the form of input boxes. Our algorithm for assuring unique
results requires a timeframe, thus the two relevant query fields are required - 
all other fields are deemed optional. Query fields that specify operations
specifically on the user's own videos are deliberately omitted as this is
not within the scope of the tool.

Before queries are dispatched to the task workers and any real work is
performed, they are validated by issuing a small version of the query for
verification only. The user is immediately notified if any of the given query
parameters create an invalid combination. This will prove invaluable to new
users who can safely learn to use the YouTube API, as well as preventing
the storage of invalid queries.
%TODO: reword?


\subsubsection{Task Page}
Stored queries may be executed on the Task page. The user selects the task he
wants performed and a query which defines the set of videos on which to operate.
The first operation will always be the fetching of video IDs, as the rest of the
available tasks depend on this. Multiple tasks may be launched in parallell.
Progress bars are updated in real-time. %TODO: too short and list-y. Reword?


\subsubsection{Result page}
The Result page contains selected statistics for the dataset returned by a given
query.
...
Due to the heavy work required to aggregate the necessary data, each section of
the page loads asynchronously on demand.

Of particular note is the graph showing the intersection between datasets. This
allows the user to quickly identify closely related queries, and can be helpful
in e.g. parameter studies.
%TODO: finish
% All of the commented text below is superfluous, though some of it may be
% salvageable. We need some way to glue the above sentences together.

% On the result page we have aggregated some statistics about the videos which are
% received by a selected query. Because the aggregation can take a lot of time for
% a big dataset, every section on this pages does asynchronously load the result
% by requesting the server only if it is necessary. Per default there are three
% sections loaded on the initilization of the page. First, a summary of the
% current selected query, containing informations about how many videos were
% found, how many metadata, comments and video representations from the DASH mdp
% file are related to the found videos and stored in the database. Second, a pie
% chart and table showing the top 10 categories. Third, a table showingassociated
% queries which were created in the past but share some videos within the group of
% the current query's videos.
% In the table there is a visualisation of the videos percentage intersection,
% which allows the user to quickly identify related search queries. It might be
% relevant to see how multiple queries are connected to each other after
% manipulating only one search parameter as we will see in the experimential
% results section.

% For some numeric values such as the views, comments, likes, dislikes and
% duration we also provide statistical aggregations of the minimum, maximum,
% average and standard deviation for the selected query compared to all videos
% saved in the database.


\subsection{Server}
The server is written in python %TODO: finish


\subsubsection{Background tasks}
All requests created by the frontend are handled asynchronously by Celery and
Redis.

Celery is used as an asynchronous task queue based on distributed messages. It
is capable of distributing tasks over a potentially vast network of nodes.~\cite{architecture:celery}
Redis is a networked in-memory key-value database.~\cite{architecture:redis}
The frameworks aren't used to their full extent given our current
single-server environment, but having these frameworks already present massively
aids future expansion.

%TODO: Hugo stopped here. This entire section needs massive reworks

Since we already have a centralized server which processes all tasks created by the clients, such as searching for 
video IDs and extracting the metadata, we had to think about how to run these tasks in a non-blocking way.
This is important
because the users might want to create multiple tasks at once and the frontend should not be blocked until one long running task
is finished. In order to solve this problem, we had to utilize a task queue system and select a message broker to feed the task queue.
We have evaluated Celery as our task queue and Redis as a message broker as the most suitable packages available for python.

Celery is an asynchronous task queue based on distributed messages~\cite{architecture:celery}
Redis itself is an open-source, networked, in-memory key-value data structure server.~\cite{architecture:redis} For our
purpose we do not utilize the full capabilities of Celery and Redis, since we have one centralized server, but using
this architecture from the very beginning makes it easy to distribute our tool on multiple servers which process the individual tasks and only
have some light weight clients which request our API.
This might be important for performance improvements and further developments.

After the user has scheduled a task from the client user interface, the task is pushed to Redis and is in a pending state until a Celery worker is available to run the task in
the background. While running the tasks in the background, celery provides an interface to update the current task's state. At this point it is important to have a different 
result backend than the normal database where we want to store the received data from the YouTube API. Because each task updates his own state after every request to the YouTube API 
in order to provide the client real time informations about the current state of a task. This results in multiple updates every second for a single task where there 
might be multiple tasks running in concurrency. Using the database on the disk as the result backend would lead to a high
disk I/O only for sharing the task's state with the client and block the saving of the data received by the YouTube API which would dramatically slow down the complete process.
Since we already have Redis as our broker system with an in-memory data structure, we use it as the result backend for our tasks and only store the tasks informations 
in the database persistently after the task has finished.







