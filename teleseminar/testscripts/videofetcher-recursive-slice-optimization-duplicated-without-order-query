#!/usr/bin/python

import Queue
import threading
import time
import urllib2, urllib, ssl
import json
import datetime
from elasticsearch import Elasticsearch

#import _strptime, otherwise we get "AttributeError: _strptime" error in the threads
import _strptime

exitFlag = 0

DEVELOPER_KEY = "AIzaSyBlO0GfmL5LuRJoVlRhMVM8VjViE5BAAs8"
numberThreads = 100

threadList = []
timeFrameList = []

totalFrame = 86400*24
videoIDs = {}
globalTimetable = {}
pageToken = ""
publishedBefore = datetime.datetime.utcnow().replace(hour=0,minute=0,second=0,microsecond=0) -datetime.timedelta(days=0)
publishedAfter = publishedBefore - datetime.timedelta(days=365)
totalFrame = int((publishedBefore-publishedAfter).total_seconds())
secondsPerFrame = int(totalFrame/numberThreads)
countAll = 0
pageAll = 0
countRequests = 0
#es = Elasticsearch()

def calculateTimeframe(publishedBefore,frame):
    return publishedBefore - datetime.timedelta(seconds=frame)


class myThread (threading.Thread):
    def __init__(self, threadID, name, q):
        threading.Thread.__init__(self)
        self.threadID = threadID
        self.name = name
        self.q = q
        self.es = Elasticsearch()
    def run(self):
        makeRequest(self.name, self.q, self.es)

def formatDate(date):
    return date.strftime('%FT%TZ')

def buildRequest(publishedAfter,publishedBefore,pageToken):
    return "https://www.googleapis.com/youtube/v3/search?part=snippet&maxResults=50&pageToken="+pageToken+"&publishedAfter="+formatDate(publishedAfter)+"&publishedBefore="+formatDate(publishedBefore)+"&type=video&key=AIzaSyBlO0GfmL5LuRJoVlRhMVM8VjViE5BAAs8"

def printDateTuple(dateTuple):
    print dateTuple[0].strftime('%FT%TZ')+"\t"+dateTuple[1].strftime('%FT%TZ')

def req(es,publishedAfter,publishedBefore,pageToken="",page=0,results=0):
        
	global countAll, pageAll, videoIDs, countRequests, workQueue, globalTimetable
	countRequests += 1
	request = buildRequest(publishedAfter,publishedBefore,pageToken)
	try: 
		result = urllib2.urlopen(request,timeout=2)
	except urllib2.URLError as e:
		if hasattr(e,"reason"):
			print e.reason
			doc = {
				"reason": e.reason,
				"request": request,
			}
		elif hasattr(e,"code"):
			print e.code
			doc = {
				"error_code": e.code,
				"request": request,
			}
		es.index(index="youtubetestindexerror",doc_type="URLError",body=doc)
	except urllib2.HTTPError as e:
		doc = {
			"error_code": e.code,
			"request": request,
		}
		es.index(index="youtubetestindexerror",doc_type="HTTPError",body=doc)
	except ssl.SSLError as e:
		print request
    
	else: 
		jsonResult = json.loads(result.read())
		if "items" in jsonResult:
			req_results = len(jsonResult['items'])
			results += req_results
                        countAll += req_results
			#secondsTimeSpan = int((publishedBefore-publishedAfter).total_seconds())
                        #if str(secondsTimeSpan) not in globalTimetable:
                        #    globalTimetable[str(secondsTimeSpan)]=1
                        #    print str(secondsTimeSpan)+"\t"+request
                        #else:
                        #    globalTimetable[str(secondsTimeSpan)] = globalTimetable[str(secondsTimeSpan)]+1
			for item in jsonResult['items']:
				videoIDs[str(item['id']['videoId'])]=1
				doc = {
					"title": item['snippet']['title'],
					"channelId": item['snippet']['channelId'],
					"description": item['snippet']['description'],
					"channelTitle": item['snippet']['channelTitle'],
					"publishedAt": item['snippet']['publishedAt'],
					"timestamp": datetime.datetime.now(),
				}
				es.index(index="youtubetestindex9",doc_type="video",id=item['id']['videoId'],body=doc)
                                
                                
			
			#slice the timeframe if has more pages and more results than 50
			if "nextPageToken" in jsonResult and req_results==50: #and secondsTimeSpan > 30:
				page += 1
				#req(publishedAfter,publishedBefore,jsonResult['nextPageToken'],page,results)

                                #adjust the publishedBefore date to the last item result in order to minimize duplicated items
                                #publishedBefore = datetime.datetime.strptime(jsonResult['items'][len(jsonResult['items'])-1]['snippet']['publishedAt'],"%Y-%m-%dT%H:%M:%S.%fZ")-datetime.timedelta(seconds=1)

				midDate = publishedAfter+(publishedBefore-publishedAfter)/2
				workQueue.put((publishedAfter,midDate,""))
				workQueue.put((midDate,publishedBefore,""))

			#else:
				#print str(publishedAfter)+"\t"+str(publishedBefore)+"\t"+str(results)+"\t"+str(page)
				#countAll += results
				#pageAll += page

def makeRequest(threadName, q,es, pageToken="", page=0, results=0):
    while not exitFlag or not workQueue.empty():
        queueLock.acquire()
        if not workQueue.empty():
            timeFrame = q.get()
            queueLock.release()
            publishedBefore = timeFrame[1]
            publishedAfter = timeFrame[0]
            pageToken = timeFrame[2]
            req(es,publishedAfter,publishedBefore,pageToken)
            
        else:
            queueLock.release()

for x in xrange(0,numberThreads):
    threadList.append(x)

tempTime = publishedBefore

for x in xrange(0,totalFrame/secondsPerFrame):
    tempTimeBefore = calculateTimeframe(tempTime,x*secondsPerFrame)
    tempTimeAfter = calculateTimeframe(tempTimeBefore,secondsPerFrame)
    #create disjunct timeframes, otherwise there are duplicated videoIDs
    tempTime = calculateTimeframe(tempTime,1)

    #for y in xrange(0,10):
    #    timeFrameList.append((tempTimeAfter,tempTimeBefore))
    timeFrameList.append((tempTimeAfter,tempTimeBefore,""))

queueLock = threading.Lock()
workQueue = Queue.Queue(0)
threads = []
threadID = 0

# Create new threads
for tName in threadList:
    thread = myThread(threadID, tName, workQueue)
    thread.start()
    threads.append(thread)
    threadID += 1

# Fill the queue
queueLock.acquire()
for timeFrame in timeFrameList:
    workQueue.put(timeFrame)

starttime = datetime.datetime.now()
queueLock.release()

# Notify threads it's time to exit
exitFlag = 1

# Wait for all threads to complete
for t in threads:
    t.join()

print globalTimetable
print str(totalFrame)+"\t"+str(secondsPerFrame)+"\t"+str(countAll)+"\t"+str(len(videoIDs))+"\t"+str(pageAll)+"\t"+str(countRequests)+"\t"+str((datetime.datetime.now()-starttime).total_seconds())

